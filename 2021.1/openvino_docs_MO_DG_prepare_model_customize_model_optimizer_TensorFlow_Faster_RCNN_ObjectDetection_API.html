<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.18 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.18"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Converting Faster R-CNN models, created with TensorFlow Object Detection API - OpenVINO™ Toolkit</title>
<!-- <script type="text/javascript" src="jquery.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script> -->
<!-- JQuery 2.2.4 -->
<script src="jquery-2.2.4.min.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<script type="text/javascript" src="dynsections.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<!-- Bootstrap 4.4.1 -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet"
    type="text/css" />
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js?v=405cebc51c254c8b400f2a138dc8f9b3"
    type="text/javascript"></script>
<!--  -->
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="/assets/versions_raw.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<script type="text/javascript" src="/assets/openvino-versions_2.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<script type="text/javascript" src="openvino-layout.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.6.2/viewer.css" integrity="sha512-Fd7AWkSYX3RBadbJlBrETnOyX5tkcQu2oN+nO11c+kewdhPlNWQ1Hj83W3hN1odgyh+k7rpvvv7IxZq9bRomCg==" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.6.2/viewer.min.js?v=405cebc51c254c8b400f2a138dc8f9b3" integrity="sha512-VzJLwaOOYyQemqxRypvwosaCDSQzOGqmBFRrKuoOv7rF2DZPlTaamK1zadh7i2FRmmpdUPAE/VBkCwq2HKPSEQ==" crossorigin="anonymous"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname">
     <a href="<domain_placeholder>" class="homelink-id">
       <img src="/assets/images/int-openvino-wht.svg" alt="OpenVINO™ Toolkit">
       <p>Documentation</p>
      </a>
    </div>
  </div>
</div>
<div id="secondnav">
    <div id="versionsSelector" class="nav-placeholder"></div>
    <div id="download-link" class="nav-placeholder"></div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.18 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js?v=be771e6d3b47d3e8b7f9ce6a7ab6b839"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Converting Faster R-CNN models, created with TensorFlow Object Detection API </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This is a deprecated page. Please, consider reading <a class="el" href="openvino_docs_MO_DG_prepare_model_convert_model_tf_specific_Convert_Object_Detection_API_Models.html">this</a> page describing new approach to convert Object Detection API models giving closer to TensorFlow inference results.</p>
<h2>Converting models created with TensorFlow Object Detection API version equal or higher than 1.6.0</h2>
<p>This chapter describes how to convert selected Faster R-CNN models from the TensorFlow Object Detection API zoo version equal or higher than 1.6.0. The full list of supported models is provided in the table below. Note that currently batch size 1 is supported only. The only Inference Engine plugin supporting these topologies inference is CPU.</p>
<p>The Faster R-CNN models contain several building blocks similar to building blocks from SSD models so it is highly recommended to read chapter about <a class="el" href="openvino_docs_MO_DG_prepare_model_customize_model_optimizer_TensorFlow_SSD_ObjectDetection_API.html">enabling TensorFlow Object Detection API SSD models</a> first. Detailed information about Faster R-CNN topologies is provided <a href="https://arxiv.org/abs/1506.01497">here</a>.</p>
<p>The TensorFlow network consists of a number of big blocks grouped by scope:</p>
<ul>
<li><code>Preprocessor</code> performs scaling/resizing of the image and converts input data to [0, 1] interval. Has two outputs: the first one is modified input image and the second one is a constant tensor with shape (batch_size, 3) and values (resized_image_height, resized_image_width, 3).</li>
<li><code>FirstStageFeatureExtractor</code> is a backbone feature extractor.</li>
<li><code>FirstStageBoxPredictor</code> calculates boxes and classes predictions.</li>
<li><code>GridAnchorGenerator</code> generates anchors coordinates.</li>
<li><code>ClipToWindow</code> crops anchors to the resized image size.</li>
<li><code>Decode</code> decodes coordinates of boxes using anchors and data from the <code>FirstStageBoxPredictor</code>.</li>
<li><code>BatchMultiClassNonMaxSuppression</code> performs non maximum suppression.</li>
<li><code>map</code> scales coordinates of boxes to [0, 1] interval by dividing coordinates by (resized_image_height, resized_image_width).</li>
<li><code>map_1</code> scales coordinates from [0, 1] interval to resized image sizes.</li>
<li><code>SecondStageFeatureExtractor</code> is a feature extractor for predicted Regions of interest (ROIs).</li>
<li><code>SecondStageBoxPredictor</code> refines box coordinates according <code>SecondStageFeatureExtractor</code>.</li>
<li><code>SecondStagePostprocessor</code> is Detection Output layer performing final boxes predictions.</li>
</ul>
<h3>Sub-graph replacements</h3>
<p>There are three sub-graph replacements defined in the <code>extensions/front/tf/legacy_faster_rcnn_support.json</code> used to convert these models:</p>
<ul>
<li>the first one replaces the <code>Preprocessor</code> block. The implementation of this replacer is in the <code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf/Preprocessor.py</code></li>
<li>the second one replaces a number of blocks in the the graph including <code>GridAnchorGenerator</code>, <code>ClipToWindow</code>, <code>Decode</code>, <code>BatchMultiClassNonMaxSuppression</code>, <code>Tile</code>, <code>Tile_1</code> and <code>map</code> with Proposal and ROIRooling layers and some additional layers to pre-process input data</li>
<li>the third one replaces <code>SecondStagePostprocessor</code> with a DetectionOutput layer.</li>
</ul>
<p>The second replacer is defined using the following configuration that matches sub-graph by points:</p>
<div class="fragment"><div class="line">{</div>
<div class="line">    &quot;custom_attributes&quot;: {</div>
<div class="line">        &quot;nms_threshold&quot;: 0.7,</div>
<div class="line">        &quot;feat_stride&quot;: 16,</div>
<div class="line">        &quot;max_proposals&quot;: 100,</div>
<div class="line">        &quot;anchor_base_size&quot;: 256,</div>
<div class="line">        &quot;anchor_scales&quot;: [0.25, 0.5, 1.0, 2.0],</div>
<div class="line">        &quot;anchor_aspect_ratios&quot;: [0.5, 1.0, 2.0],</div>
<div class="line">        &quot;roi_spatial_scale&quot;: 0.0625</div>
<div class="line">    },</div>
<div class="line">    &quot;id&quot;: &quot;TFObjectDetectionAPIFasterRCNNProposalAndROIPooling&quot;,</div>
<div class="line">    &quot;include_inputs_to_sub_graph&quot;: true,</div>
<div class="line">    &quot;include_outputs_to_sub_graph&quot;: true,</div>
<div class="line">    &quot;instances&quot;: {</div>
<div class="line">        &quot;end_points&quot;: [</div>
<div class="line">            &quot;CropAndResize&quot;,</div>
<div class="line">            &quot;map_1/TensorArrayStack/TensorArrayGatherV3&quot;,</div>
<div class="line">            &quot;map_1/while/strided_slice/Enter&quot;,</div>
<div class="line">            &quot;BatchMultiClassNonMaxSuppression/map/TensorArrayStack_4/TensorArrayGatherV3&quot;</div>
<div class="line">        ],</div>
<div class="line">        &quot;start_points&quot;: [</div>
<div class="line">            &quot;FirstStageBoxPredictor/concat&quot;,</div>
<div class="line">            &quot;FirstStageBoxPredictor/concat_1&quot;,</div>
<div class="line">            &quot;GridAnchorGenerator/Identity&quot;,</div>
<div class="line">            &quot;Shape&quot;,</div>
<div class="line">            &quot;CropAndResize&quot;</div>
<div class="line">        ]</div>
<div class="line">    },</div>
<div class="line">    &quot;match_kind&quot;: &quot;points&quot;</div>
<div class="line">}</div>
</div><!-- fragment --><p>The <code>start_points</code> list contains the following nodes:</p>
<ul>
<li><code>FirstStageBoxPredictor/concat</code> node produces box coordinates predictions.</li>
<li><code>FirstStageBoxPredictor/concat_1</code> node produces classes predictions which will be used for the ROIs</li>
<li><code>GridAnchorGenerator/Identity</code> node produces anchors coordinates.</li>
<li><code>Shape</code> and <code>CropAndResize</code> nodes are specified as inputs to correctly isolate the required sub-graph. Refer to the <a class="el" href="openvino_docs_MO_DG_prepare_model_customize_model_optimizer_Subgraph_Replacement_Model_Optimizer.html">chapter</a> for more information about replacements by points.</li>
</ul>
<p>The <code>end_points</code> list contains the following nodes:</p>
<ul>
<li><code>CropAndResize</code> is the node that performs ROI pooling operation.</li>
<li><code>map_1/TensorArrayStack/TensorArrayGatherV3</code>, <code>map_1/while/strided_slice/Enter</code> and <code>BatchMultiClassNonMaxSuppression/map/TensorArrayStack_4/TensorArrayGatherV3</code> are specified to correctly isolate the sub-graph.</li>
</ul>
<p>The <code>custom_attributes</code> dictionary contains attributes where most values are taken from the topology-specific configuration file <code>samples/configs/faster_rcnn_*.config</code> of the <a href="https://github.com/tensorflow/models/tree/master/research/object_detection">TensorFlow Object Detection API repository</a>:</p>
<ul>
<li><code>nms_threshold</code> is the value of the <code>first_stage_nms_iou_threshold</code> parameter.</li>
<li><code>feat_stride</code> is the value of the <code>height_stride</code> and <code>width_stride</code> parameters. Inference Engine supports case when these two values are equal that is why the replacement configuration file contains just one parameter.</li>
<li><code>max_proposals</code> is the value of the <code>max_total_detections</code> parameter which is a maximum number of proposal boxes from the Proposal layer and detected boxes.</li>
<li><code>anchor_base_size</code> is the base size of the generated anchor. The 256 is the default value for this parameter and it is not specified in the configuration file.</li>
<li><code>anchor_scales" is the value of the</code>scales` attrbite.</li>
<li><code>anchor_aspect_ratios</code> is the value of the <code>aspect_ratios</code> attribute.</li>
<li><code>roi_spatial_scale</code> is needed for the Inference Engine ROIPooling layer. It is the default value that is not actually used.</li>
</ul>
<p>The identifier for this replacer is <code>TFObjectDetectionAPIFasterRCNNProposalAndROIPooling</code>. The Python implementation of this replacer is in the file <code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf/FasterRCNNs.py</code>.</p>
<p>The first four functions of the replacer class are the following:</p>
<div class="fragment"><div class="line">class TFObjectDetectionAPIFasterRCNNProposalAndROIPooling(FrontReplacementFromConfigFileSubGraph):</div>
<div class="line">    &quot;&quot;&quot;</div>
<div class="line">    This class replaces sub-graph of operations with Proposal and ROIPooling layers and additional layers transforming</div>
<div class="line">    tensors from layout of TensorFlow to layout required by Inference Engine.</div>
<div class="line">    Refer to comments inside the function for more information about performed actions.</div>
<div class="line">    &quot;&quot;&quot;</div>
<div class="line">    replacement_id = &#39;TFObjectDetectionAPIFasterRCNNProposalAndROIPooling&#39;</div>
<div class="line"> </div>
<div class="line">    def run_after(self):</div>
<div class="line">        return [PreprocessorReplacement]</div>
<div class="line"> </div>
<div class="line">    def run_before(self):</div>
<div class="line">        return [SecondStagePostprocessorReplacement]</div>
<div class="line"> </div>
<div class="line">    def output_edges_match(self, graph: nx.DiGraph, match: SubgraphMatch, new_sub_graph: dict):</div>
<div class="line">        return {match.output_node(0)[0].id: new_sub_graph[&#39;roi_pooling_node&#39;].id}</div>
<div class="line"> </div>
<div class="line">    def nodes_to_remove(self, graph: nx.MultiDiGraph, match: SubgraphMatch):</div>
<div class="line">        new_list = match.matched_nodes_names().copy()</div>
<div class="line">        # do not remove nodes that produce box predictions and class predictions</div>
<div class="line">        new_list.remove(match.single_input_node(0)[0].id)</div>
<div class="line">        new_list.remove(match.single_input_node(1)[0].id)</div>
<div class="line">        return new_list</div>
</div><!-- fragment --><p>The function <code>run_after</code> returns list of Python classes inherited from one of the replacer classes (<code>FrontReplacementOp</code>, <code>FrontReplacementPattern</code>, <code>FrontReplacementFromConfigFileSubGraph</code> etc) those current sub-graph replacement class must be run after. In this case the replacer must be run after the <code>Preprocessor</code> is removed by the <code>PreprocessorReplacement</code> replacer. Similar way the <code>run_before</code> function is used to tell Model Optimizer to execute <code>SecondStagePostprocessorReplacement</code> before this replacer.</p>
<p>The <code>output_edges_match</code> function describes matching between the output nodes of the sub-graph before replacement and after. In this case the only needed output node of the sub-graph is the <code>CropAndResize</code> node which is identified with <code>match.output_node(0)[0]</code>. The new output node which is created in the <code>generate_sub_graph</code> function is identified with &lsquo;new_sub_graph['roi_pooling_node&rsquo;]`.</p>
<p>The <code>nodes_to_remove</code> function takes the default list of nodes to be removed which contains all matched nodes and remove from them two input nodes which are identified with <code>match.single_input_node(0)[0]</code> and <code>match.single_input_node(1)[0]</code>. These nodes will be connected as inputs to new nodes being generated in the <code>generate_sub_graph</code> function so they should node be removed.</p>
<p>The code generating new sub-graph is the following:</p>
<div class="fragment"><div class="line">def generate_sub_graph(self, graph: nx.MultiDiGraph, match: SubgraphMatch):</div>
<div class="line">    log.debug(&#39;TFObjectDetectionAPIFasterRCNNProposal: matched_nodes = {}&#39;.format(match.matched_nodes_names()))</div>
<div class="line"> </div>
<div class="line">    config_attrs = match.custom_replacement_desc.custom_attributes</div>
<div class="line">    nms_threshold = config_attrs[&#39;nms_threshold&#39;]</div>
<div class="line">    feat_stride = config_attrs[&#39;feat_stride&#39;]</div>
<div class="line">    max_proposals = config_attrs[&#39;max_proposals&#39;]</div>
<div class="line">    anchor_base_size = config_attrs[&#39;anchor_base_size&#39;]</div>
<div class="line">    roi_spatial_scale = config_attrs[&#39;roi_spatial_scale&#39;]</div>
<div class="line">    proposal_ratios = config_attrs[&#39;anchor_aspect_ratios&#39;]</div>
<div class="line">    proposal_scales = config_attrs[&#39;anchor_scales&#39;]</div>
<div class="line">    anchors_count = len(proposal_ratios) * len(proposal_scales)</div>
</div><!-- fragment --><p>These lines get parameters defined in the sub-graph replacement configuration file and calculate initial anchors count.</p>
<div class="fragment"><div class="line"># get the ROIPool size from the CropAndResize which performs the same action</div>
<div class="line">if &#39;CropAndResize&#39; not in graph.nodes():</div>
<div class="line">    raise Error(&#39;Failed to find node with name &quot;CropAndResize&quot; in the topology. Probably this is not Faster&#39;</div>
<div class="line">                &#39; RCNN topology or it is not supported&#39;)</div>
<div class="line">roi_pool_size = Node(graph, &#39;CropAndResize&#39;).in_node(3).value[0]</div>
</div><!-- fragment --><p>The code above gets the ROI Pooling spatial output dimension size as a value from the fourth argument of the node with name <code>CropAndResize</code>.</p>
<div class="fragment"><div class="line"># Convolution/matmul node that produces classes predictions</div>
<div class="line"># Permute result of the tensor with classes permissions so it will be in a correct layout for Softmax</div>
<div class="line">predictions_node = match.single_input_node(1)[0].in_node(0).in_node(0)</div>
<div class="line">permute_predictions_op = Permute(graph, {&#39;order&#39;: np.array([0, 2, 3, 1])})</div>
<div class="line">permute_predictions_node = permute_predictions_op.create_node([], dict(name=predictions_node.name + &#39;/Permute_&#39;))</div>
<div class="line">insert_node_after(predictions_node, permute_predictions_node, 0)</div>
<div class="line"> </div>
<div class="line">reshape_classes_op = Reshape(graph, {&#39;dim&#39;: np.array([0, -1, 2])})</div>
<div class="line">reshape_classes_node = reshape_classes_op.create_node([permute_predictions_node],</div>
<div class="line">                                                      dict(name=&#39;Reshape_FirstStageBoxPredictor_Class_&#39;))</div>
<div class="line">update_attrs(reshape_classes_node, &#39;shape_attrs&#39;, &#39;dim&#39;)</div>
<div class="line"> </div>
<div class="line">softmax_conf_op = Softmax(graph, {&#39;axis&#39;: 1})</div>
<div class="line">softmax_conf_node = softmax_conf_op.create_node([reshape_classes_node],</div>
<div class="line">                                                dict(name=&#39;FirstStageBoxPredictor_SoftMax_Class_&#39;))</div>
</div><!-- fragment --><p>The output with class predictions from the <code>FirstStageBoxPredictor</code> is generated with a convolution operation. The convolution output data layout in TensorFlow is NHWC while Inference Engine uses NCHW layout. Model Optimizer by default converts the weights of TensorFlow convolutions to produce output tensor in NCHW layout required by Inference Engine. The issue arises because the class predictions tensor is passed through the Softmax operation to produce class probabilities. The Inference Engine Softmax is performed over the fastest-changing dimension which is 'W' in Inference Engine. Thus, the softmax operation will be performed over a wrong dimension after conversion of the convolution node producing classes predicitions. The solution is to add Permute and Reshape operations to prepare the input data for Softmax. The Reshape operation is required to make the size of the fastest-changing dimension equal to 2, because there are 2 classes being predicted: background and foreground.</p>
<p>Another issue is that layout of elements in the predicted classes tensor is different between TensorFlow and Inference Engine Proposal layer requirements. In TensorFlow the tensor has the following virtual layout [N, H, W, num_anchors, num_classes] while the Inference Engine Proposal layer requires in the following virtual layout [N, num_classes, num_anchors, H, W]. Thus, it is necessary to reshape, permute and then reshape again output from the Softmax to the required shape for the Proposal layer:</p>
<div class="fragment"><div class="line">reshape_softmax_op = Reshape(graph, {&#39;dim&#39;: np.array([1, anchors_count, 2, -1])})</div>
<div class="line">reshape_softmax_node = reshape_softmax_op.create_node([softmax_conf_node], dict(name=&#39;Reshape_Softmax_Class_&#39;))</div>
<div class="line">update_attrs(reshape_softmax_node, &#39;shape_attrs&#39;, &#39;dim&#39;)</div>
<div class="line"> </div>
<div class="line">permute_reshape_softmax_op = Permute(graph, {&#39;order&#39;: np.array([0, 1, 3, 2])})</div>
<div class="line">permute_reshape_softmax_node = permute_reshape_softmax_op.create_node([reshape_softmax_node],</div>
<div class="line">                                                                      dict(name=&#39;Permute_&#39;))</div>
<div class="line"> </div>
<div class="line"># implement custom reshape infer function because we need to know the input convolution node output dimension</div>
<div class="line"># sizes but we can know it only after partial infer</div>
<div class="line">reshape_permute_op = Reshape(graph, {&#39;dim&#39;: np.ones([4]), &#39;anchors_count&#39;: anchors_count,</div>
<div class="line">                                     &#39;conv_node&#39;: predictions_node})</div>
<div class="line">reshape_permute_op.attrs[&#39;old_infer&#39;] = reshape_permute_op.attrs[&#39;infer&#39;]</div>
<div class="line">reshape_permute_op.attrs[&#39;infer&#39;] = __class__.classes_probabilities_reshape_shape_infer</div>
<div class="line">reshape_permute_node = reshape_permute_op.create_node([permute_reshape_softmax_node],</div>
<div class="line">                                                      dict(name=&#39;Reshape_Permute_Class_&#39;))</div>
<div class="line">update_attrs(reshape_permute_node, &#39;shape_attrs&#39;, &#39;dim&#39;)</div>
</div><!-- fragment --><p>The Proposal layer has 3 inputs: classes probabilities, boxes predictions and a input shape of the image. The first two tensors are ready so it is necessary to create the Const operation that produces the desired third input tensor.</p>
<div class="fragment"><div class="line"># create constant input with the image height, width and scale H and scale W (if present) required for Proposal</div>
<div class="line">const_value = np.array([[input_height, input_width, 1]], dtype=np.float32)</div>
<div class="line">const_op = Const(graph, dict(value=const_value, shape=const_value.shape))</div>
<div class="line">const_node = const_op.create_node([], dict(name=&#39;Proposal_const_image_size_&#39;))</div>
</div><!-- fragment --><p>Now add the Proposal layer:</p>
<div class="fragment"><div class="line">proposal_op = ProposalOp(graph, dict(min_size=10, framework=&#39;tensorflow&#39;, box_coordinate_scale=10,</div>
<div class="line">                                     box_size_scale=5, post_nms_topn=max_proposals, feat_stride=feat_stride,</div>
<div class="line">                                     ratio=proposal_ratios, scale=proposal_scales, base_size=anchor_base_size,</div>
<div class="line">                                     pre_nms_topn=2**31 - 1,</div>
<div class="line">                                     nms_thresh=nms_threshold))</div>
<div class="line">proposal_node = proposal_op.create_node([reshape_permute_node,</div>
<div class="line">                                         match.single_input_node(0)[0].in_node(0).in_node(0),</div>
<div class="line">                                         const_node],</div>
<div class="line">                                        dict(name=proposal_op.attrs[&#39;type&#39;] + &#39;_&#39;))</div>
</div><!-- fragment --><p>The box coordinates in the TensorFlow are in the following layout "YXYX" while Inference Engine uses "XYXY" layout so it is necessary to swap coordinates produced by Proposal layer. It is implemented with help of a convolution node with a special filter of a size [5, 5]:</p>
<div class="fragment"><div class="line">proposal_reshape_4d_op = Reshape(graph, {&#39;dim&#39;: np.array([max_proposals, 1, 1, 5])})</div>
<div class="line">proposal_reshape_4d_node = proposal_reshape_4d_op.create_node([proposal_node], dict(name=&quot;reshape_4d_&quot;))</div>
<div class="line">update_attrs(proposal_reshape_4d_node, &#39;shape_attrs&#39;, &#39;dim&#39;)</div>
<div class="line"> </div>
<div class="line"># create convolution node to swap X and Y coordinates in the proposals</div>
<div class="line">conv_filter_const_data = np.array(np.array([[1, 0, 0, 0, 0],</div>
<div class="line">                                            [0, 0, 1, 0, 0],</div>
<div class="line">                                            [0, 1, 0, 0, 0],</div>
<div class="line">                                            [0, 0, 0, 0, 1],</div>
<div class="line">                                            [0, 0, 0, 1, 0]],</div>
<div class="line">                                           dtype=np.float32).reshape([1, 1, 5, 5]), dtype=np.float32)</div>
<div class="line">conv_filter_const_op = Const(graph, dict(value=conv_filter_const_data, spatial_dims=np.array([2, 3])))</div>
<div class="line">conv_filter_const_node = conv_filter_const_op.create_node([], dict(name=&quot;conv_weights&quot;))</div>
<div class="line"> </div>
<div class="line">conv_op = Op(graph, {</div>
<div class="line">                &#39;op&#39;: &#39;Conv2D&#39;,</div>
<div class="line">                &#39;bias_addable&#39;: False,</div>
<div class="line">                &#39;spatial_dims&#39;: np.array([1, 2]),</div>
<div class="line">                &#39;channel_dims&#39;: np.array([3]),</div>
<div class="line">                &#39;batch_dims&#39;: np.array([0]),</div>
<div class="line">                &#39;pad&#39;: None,</div>
<div class="line">                &#39;pad_spatial_shape&#39;: None,</div>
<div class="line">                &#39;input_feature_channel&#39;: 2,</div>
<div class="line">                &#39;output_feature_channel&#39;: 2,</div>
<div class="line">                &#39;output_shape&#39;: [max_proposals, 1, 1, 5],</div>
<div class="line">                &#39;dilation&#39;: np.array([1, 1, 1, 1], dtype=np.int64),</div>
<div class="line">                &#39;stride&#39;: np.array([1, 1, 1, 1]),</div>
<div class="line">                &#39;type&#39;: &#39;Convolution&#39;,</div>
<div class="line">                &#39;group&#39;: None,</div>
<div class="line">                &#39;layout&#39;: &#39;NHWC&#39;,</div>
<div class="line">                &#39;infer&#39;: __class__.fake_conv_shape_infer})</div>
<div class="line">predictions_node = conv_op.create_node([proposal_reshape_4d_node, conv_filter_const_node], dict(name=&quot;conv_&quot;))</div>
<div class="line">update_ie_fields(graph.node[predictions_node.id])</div>
<div class="line"> </div>
<div class="line">proposal_reshape_2d_op = Reshape(graph, {&#39;dim&#39;: np.array([max_proposals, 5])})</div>
<div class="line">proposal_reshape_2d_node = proposal_reshape_2d_op.create_node([predictions_node], dict(name=&quot;reshape_2d_&quot;))</div>
<div class="line"># set specific name for this Reshape operation so we can use it in the DetectionOutput replacer</div>
<div class="line">proposal_reshape_2d_node[&#39;name&#39;] = &#39;swapped_proposals&#39;</div>
</div><!-- fragment --><p>The ROIPooling layer in TensorFlow is implemented with operation called <code>CropAndResize</code> with bi-linear filtration. Inference Engine implementation of the ROIPooling layer with bi-linear filtration requires input boxes coordinates be scaled to [0, 1] interval. Adding elementwise multiplication of box coordinates solves this issue:</p>
<div class="fragment"><div class="line"># the TF implementation of Proposal with bi-linear filtration need proposals scaled by image size</div>
<div class="line">proposal_scale_const = np.array([1.0, 1 / input_height, 1 / input_width, 1 / input_height, 1 / input_width],</div>
<div class="line">                                dtype=np.float32)</div>
<div class="line">proposal_scale_const_op = Const(graph, dict(value=proposal_scale_const, shape=proposal_scale_const.shape))</div>
<div class="line">proposal_scale_const_node = proposal_scale_const_op.create_node([], dict(name=&#39;Proposal_scale_const_&#39;))</div>
<div class="line"> </div>
<div class="line">scale_proposals_op = Eltwise(graph, {&#39;operation&#39;: &#39;mul&#39;})</div>
<div class="line">scale_proposals_node = scale_proposals_op.create_node([proposal_reshape_2d_node, proposal_scale_const_node],</div>
<div class="line">                                                      dict(name=&#39;scale_proposals_&#39;))</div>
</div><!-- fragment --><p>The last step is to create the ROIPooling node with 2 inputs: the identified feature maps from the <code>FirstStageFeatureExtractor</code> and the scaled output of the Proposal layer:</p>
<div class="fragment"><div class="line">feature_extractor_output_nodes = scope_output_nodes(graph, &#39;FirstStageFeatureExtractor&#39;)</div>
<div class="line">if len(feature_extractor_output_nodes) != 1:</div>
<div class="line">    raise Error(&quot;Failed to determine FirstStageFeatureExtractor output node to connect it to the ROIPooling.&quot;</div>
<div class="line">                &quot;Found the following nodes: {}&quot;.format([node.name for node in feature_extractor_output_nodes]))</div>
<div class="line"> </div>
<div class="line">roi_pooling_op = ROIPooling(graph, dict(method=&quot;bilinear&quot;, framework=&quot;tensorflow&quot;,</div>
<div class="line">                                        pooled_h=roi_pool_size, pooled_w=roi_pool_size,</div>
<div class="line">                                        spatial_scale=roi_spatial_scale))</div>
<div class="line">roi_pooling_node = roi_pooling_op.create_node([feature_extractor_output_nodes[0], scale_proposals_node],</div>
<div class="line">                                              dict(name=&#39;ROI_Pooling_&#39;))</div>
<div class="line"> </div>
<div class="line">return {&#39;roi_pooling_node&#39;: roi_pooling_node}</div>
</div><!-- fragment --><p>The are two additional methods implemented in the replacer class:</p>
<ul>
<li>The <code>fake_conv_shape_infer</code> is a silly infer function for the convolution that permutes X and Y coordinates of the Proposal output which avoids setting a lot of internal attributes required for propoper shape inference.</li>
<li>The "classes_probabilities_reshape_shape_infer" function is used to update the output dimension of the reshape operation. The output spatial dimensions depends on the convolution output spatial dimensions thus they are not known until the shape inference pass which is performed after this sub-graph replacement class. So this custom infer function is called instead of default Reshape shape inference function, updates the required attribute "dim" of the node with the convolution output spatial dimensions which are known at the time of calling this inference function and then call the default Reshape inference function.</li>
</ul>
<div class="fragment"><div class="line">@staticmethod</div>
<div class="line">def fake_conv_shape_infer(node: Node):</div>
<div class="line">    node.out_node(0).shape = node.in_node(0).shape</div>
<div class="line">    # call functions to update internal attributes required for correct IR generation</div>
<div class="line">    mark_input_bins(node)</div>
<div class="line">    assign_dims_to_weights(node.in_node(1), [0, 1], node.input_feature_channel, node.output_feature_channel, 4)</div>
<div class="line"> </div>
<div class="line">@staticmethod</div>
<div class="line">def classes_probabilities_reshape_shape_infer(node: Node):</div>
<div class="line">    # now we can determine the reshape dimensions from Convolution node</div>
<div class="line">    conv_node = node.conv_node</div>
<div class="line">    conv_output_shape = conv_node.out_node().shape</div>
<div class="line"> </div>
<div class="line">    # update desired shape of the Reshape node</div>
<div class="line">    node.dim = np.array([0, conv_output_shape[1], conv_output_shape[2], node.anchors_count * 2])</div>
<div class="line">    node.old_infer(node)</div>
</div><!-- fragment --><p>The second replacer defined in the sub-graph replacement configuration file replaces the <code>SecondStagePostprocessor</code> block and is defined using scope:</p>
<div class="fragment"><div class="line">{</div>
<div class="line">    &quot;custom_attributes&quot;: {</div>
<div class="line">        &quot;code_type&quot;: &quot;caffe.PriorBoxParameter.CENTER_SIZE&quot;,</div>
<div class="line">        &quot;confidence_threshold&quot;: 0.01,</div>
<div class="line">        &quot;keep_top_k&quot;: 300,</div>
<div class="line">        &quot;nms_threshold&quot;: 0.6,</div>
<div class="line">        &quot;pad_mode&quot;: &quot;caffe.ResizeParameter.CONSTANT&quot;,</div>
<div class="line">        &quot;resize_mode&quot;: &quot;caffe.ResizeParameter.WARP&quot;,</div>
<div class="line">        &quot;max_detections_per_class&quot;: 100,</div>
<div class="line">        &quot;num_classes&quot;: 90</div>
<div class="line">    },</div>
<div class="line">    &quot;id&quot;: &quot;SecondStagePostprocessorReplacement&quot;,</div>
<div class="line">    &quot;inputs&quot;: [</div>
<div class="line">        [</div>
<div class="line">            {</div>
<div class="line">                &quot;node&quot;: &quot;Reshape$&quot;,</div>
<div class="line">                &quot;port&quot;: 0</div>
<div class="line">            }</div>
<div class="line">        ],</div>
<div class="line">        [</div>
<div class="line">            {</div>
<div class="line">                &quot;node&quot;: &quot;Reshape_1$&quot;,</div>
<div class="line">                &quot;port&quot;: 0</div>
<div class="line">            }</div>
<div class="line">        ],</div>
<div class="line">        [</div>
<div class="line">            {</div>
<div class="line">                &quot;node&quot;: &quot;ExpandDims$&quot;,</div>
<div class="line">                &quot;port&quot;: 0</div>
<div class="line">            }</div>
<div class="line">        ]</div>
<div class="line">    ],</div>
<div class="line">    &quot;instances&quot;: [</div>
<div class="line">        &quot;.*SecondStagePostprocessor/&quot;</div>
<div class="line">    ],</div>
<div class="line">    &quot;match_kind&quot;: &quot;scope&quot;,</div>
<div class="line">    &quot;outputs&quot;: [</div>
<div class="line">        {</div>
<div class="line">            &quot;node&quot;: &quot;BatchMultiClassNonMaxSuppression/map/TensorArrayStack/TensorArrayGatherV3$&quot;,</div>
<div class="line">            &quot;port&quot;: 0</div>
<div class="line">        }</div>
<div class="line">    ]</div>
<div class="line">}</div>
</div><!-- fragment --><p>The replacement code is similar to the <code>SecondStagePostprocessor</code> replacement for the SSDs topologies. The are two major difference:</p>
<ul>
<li>The tensor with bounding boxes doesn't contain locations for class 0 (background class) but Inference Engine Detection Output layer requires it. The Const node with some dummy values are created and concatenated with the tensor.</li>
<li>The priors tensor is not constant like in SSDs so the bounding boxes tensor must be scaled with variances [0.1, 0.1, 0.2, 0.2].</li>
</ul>
<p>The descibed above difference are resolved with the following code:</p>
<div class="fragment"><div class="line"># TF produces locations tensor without boxes for background.</div>
<div class="line"># Inference Engine DetectionOutput layer requires background boxes so we generate them with some values</div>
<div class="line"># and concatenate with locations tensor</div>
<div class="line">fake_background_locs_blob = np.tile([[[1, 1, 2, 2]]], [max_detections_per_class, 1, 1])</div>
<div class="line">fake_background_locs_const_op = Const(graph, dict(value=fake_background_locs_blob,</div>
<div class="line">                                                  shape=fake_background_locs_blob.shape))</div>
<div class="line">fake_background_locs_const_node = fake_background_locs_const_op.create_node([])</div>
<div class="line"> </div>
<div class="line">reshape_loc_op = Reshape(graph, {&#39;dim&#39;: np.array([max_detections_per_class, num_classes, 4])})</div>
<div class="line">reshape_loc_node = reshape_loc_op.create_node([match.single_input_node(0)[0].in_node(0)],</div>
<div class="line">                                              dict(name=&#39;Reshape_loc_&#39;))</div>
<div class="line"> </div>
<div class="line">concat_loc_op = Concat(graph, {&#39;axis&#39;: 1})</div>
<div class="line">concat_loc_node = concat_loc_op.create_node([fake_background_locs_const_node, reshape_loc_node],</div>
<div class="line">                                            dict(name=&#39;Concat_fake_loc_&#39;))</div>
<div class="line"> </div>
<div class="line"># blob with variances</div>
<div class="line">variances_blob = np.array([0.1, 0.1, 0.2, 0.2])</div>
<div class="line">variances_const_op = Const(graph, dict(value=variances_blob, shape=variances_blob.shape))</div>
<div class="line">variances_const_node = variances_const_op.create_node([])</div>
<div class="line"> </div>
<div class="line"># reshape locations tensor to 2D so it could be passed to Eltwise which will be converted to ScaleShift</div>
<div class="line">reshape_loc_2d_op = Reshape(graph, {&#39;dim&#39;: np.array([-1, 4])})</div>
<div class="line">reshape_loc_2d_node = reshape_loc_2d_op.create_node([concat_loc_node], dict(name=&#39;reshape_locs_2d_&#39;))</div>
<div class="line"> </div>
<div class="line"># element-wise multiply locations with variances</div>
<div class="line">eltwise_locs_op = Eltwise(graph, {&#39;operation&#39;: &#39;mul&#39;})</div>
<div class="line">eltwise_locs_node = eltwise_locs_op.create_node([reshape_loc_2d_node, variances_const_node],</div>
<div class="line">                                                dict(name=&#39;scale_locs_&#39;))</div>
</div><!-- fragment --><h3>Example of Model Optimizer Command-Line for TensorFlow's Faster R-CNNs</h3>
<p>The final command line to convert Faster R-CNNs from the TensorFlow* Object Detection Zoo is the following:</p>
<div class="fragment"><div class="line">./mo.py --input_model=&lt;path_to_frozen.pb&gt; --output=detection_boxes,detection_scores,num_detections --tensorflow_use_custom_operations_config extensions/front/tf/legacy_faster_rcnn_support.json</div>
</div><!-- fragment --><p>Note that there are minor changes that should be made to the and sub-graph replacement configuration file <code>&lt;INSTALL_DIR&gt;/deployment_tools/model_optimizer/extensions/front/tf/legacy_faster_rcnn_support.json</code> before converting particular Faster R-CNN topology. Refer to the table below.</p>
<h3>Sub-Graph Replacement Configuration File Parameters to Convert Different Faster R-CNN Models</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadLeft">Model Name </th><th class="markdownTableHeadCenter">Configuration File Changes  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">faster_rcnn_inception_v2_coco </td><td class="markdownTableBodyCenter">None  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft">faster_rcnn_resnet50_coco </td><td class="markdownTableBodyCenter">None  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">faster_rcnn_resnet50_lowproposals_coco </td><td class="markdownTableBodyCenter">None  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft">faster_rcnn_resnet101_coco </td><td class="markdownTableBodyCenter">None  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">faster_rcnn_resnet101_lowproposals_coco </td><td class="markdownTableBodyCenter">None  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyLeft">faster_rcnn_inception_resnet_v2_atrous_coco </td><td class="markdownTableBodyCenter">"feat_stride: 8"  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyLeft">faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco </td><td class="markdownTableBodyCenter">"feat_stride: 8"  </td></tr>
</table>
</div></div><!-- contents -->
</div><!-- PageDoc -->
    <div class="footer">
      <div id="nav-path" class="navpath"></div>
      <div class="footer-content">
        <div class="opt-notice">For more complete information about compiler optimizations, see our 
        <a class="el" href="https://software.intel.com/articles/optimization-notice">Optimization Notice</a>
        </div>
        <div class="footer-row">
          <ul>
            <li><a href="https://software.intel.com/en-us/forums/computer-vision">Support</a></li>
            <li><a href="/<version_placeholder>/openvino_docs_Legal_Information.html">Legal Information</a></li>
            <li><a href="https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html">Cookies and Similar Technologies Notice</a></li>
          </ul>
        </div>
      </div>
    </div>
  </body>
</html>