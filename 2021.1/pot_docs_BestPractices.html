<!DOCTYPE html>
<html lang="en">
<head>
<!-- HTML header for doxygen 1.8.18 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.18"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Post-training optimization best practices - OpenVINO™ Toolkit</title>
<!-- <script type="text/javascript" src="jquery.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script> -->
<!-- JQuery 2.2.4 -->
<script src="jquery-2.2.4.min.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<script type="text/javascript" src="dynsections.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<!-- Bootstrap 4.4.1 -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet"
    type="text/css" />
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js?v=405cebc51c254c8b400f2a138dc8f9b3"
    type="text/javascript"></script>
<!--  -->
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="/assets/versions_raw.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<script type="text/javascript" src="/assets/openvino-versions_2.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<script type="text/javascript" src="openvino-layout.js?v=405cebc51c254c8b400f2a138dc8f9b3"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.6.2/viewer.css" integrity="sha512-Fd7AWkSYX3RBadbJlBrETnOyX5tkcQu2oN+nO11c+kewdhPlNWQ1Hj83W3hN1odgyh+k7rpvvv7IxZq9bRomCg==" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/viewerjs/1.6.2/viewer.min.js?v=405cebc51c254c8b400f2a138dc8f9b3" integrity="sha512-VzJLwaOOYyQemqxRypvwosaCDSQzOGqmBFRrKuoOv7rF2DZPlTaamK1zadh7i2FRmmpdUPAE/VBkCwq2HKPSEQ==" crossorigin="anonymous"></script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
  <div id="projectalign">
   <div id="projectname">
     <a href="<domain_placeholder>" class="homelink-id">
       <img src="/assets/images/int-openvino-wht.svg" alt="OpenVINO™ Toolkit">
       <p>Documentation</p>
      </a>
    </div>
  </div>
</div>
<div id="secondnav">
    <div id="versionsSelector" class="nav-placeholder"></div>
    <div id="download-link" class="nav-placeholder"></div>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.18 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js?v=be771e6d3b47d3e8b7f9ce6a7ab6b839"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Post-training optimization best practices </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This document describes the most important insights about model optimization using the Post-training Optimization Toolkit (POT). The post-training optimization usually is the fastest way to get a low-precision model because it does not require a fine-tuning and thus, there is no need in the training dataset, pipeline and availability of the powerful training hardware. In some cases, it may lead to not satisfactory accuracy drop, especially when quantizing the whole model. However, it can be still helpful for fast performance evaluation in order to understand the possible speed up from low precision. Before going into details we suggest reading the following <a class="el" href="pot_README.html">POT documentation</a>.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: It is recommended to use the target hardware for quantization, but if it is not possible, the best option is the CPU with the same instruction set or a VNNI-based CPU if the target device is GPU or VPU. Deep Learning Inference Engine uses highly optimized low-level operations which fully utilize available </p>
</blockquote>
<p>instruction set. Due to this, some issues that appear on one type of hardware, </p><blockquote class="doxtable">
<p>like intermediate results integer saturation, may not appear on hardware with more advanced instruction set. This means that a model generated and evaluated on non-target hardware may have different both accuracy and latency when moved to target hardware. </p>
</blockquote>
<h2>Starting Post-Training Optimization</h2>
<p>The POT has lots of knobs that can be used to get an accurate quantized model. However, as a starting point we suggest using the <code>DefaultQuantization</code> algorithm with default settings. In many cases it leads to satisfied accuracy and performance speedup. A fragment of the configuration file (<code>config/default_quantization_template.json</code> in the POT directory) with default settings is shown below: </p><div class="fragment"><div class="line">&quot;compression&quot;: {</div>
<div class="line">  &quot;target_device&quot;: &quot;ANY&quot;, // Target device, the specificity of which will be taken into account during optimization.</div>
<div class="line">                          // The default value &quot;ANY&quot; stands for compatible quantization supported by any HW.</div>
<div class="line">  &quot;algorithms&quot;: [</div>
<div class="line">    {</div>
<div class="line">      &quot;name&quot;: &quot;DefaultQuantization&quot;, // Optimization algorithm name</div>
<div class="line">      &quot;params&quot;: {</div>
<div class="line">        &quot;preset&quot;: &quot;performance&quot;, // Preset [performance, mixed] which control the quantization</div>
<div class="line">                                 // mode (symmetric, mixed (weights symmetric and activations asymmetric)</div>
<div class="line">                                 // and fully asymmetric respectively)</div>
<div class="line"> </div>
<div class="line">        &quot;stat_subset_size&quot;: 300  // Size of subset to calculate activations statistics that can be used</div>
<div class="line">                                 // for quantization parameters calculation</div>
<div class="line">      }</div>
<div class="line">    }</div>
<div class="line">  ]</div>
<div class="line">}</div>
</div><!-- fragment --><p>The <code>DefaultQuantization</code> algorithm implies two different usage scenarios (<em>engines</em>):</p><ul>
<li>With <b>AccuracyChecker</b> under the hood. Here the POT relies on the AccuracyChecker execution flow so that all the required settings (the same as for FP32 model) must be provided in order to get correct results for the model.</li>
<li>As a part of <b>Simplified mode</b> with minimum configuration effort. Currently, this mode can be used when the model or dataset are not supported by the AccuracyChecker tool but it has several limitations:<ul>
<li>The model must be from Computer Vision domain.</li>
<li>The model must have only one input (exceptions are Faster and Mask R-CNN models).</li>
</ul>
</li>
</ul>
<blockquote class="doxtable">
<p><b>NOTE</b>: The first scenario potentially gives better accurate results and can handle more models while the second is much easier to use because </p>
</blockquote>
<p>it does not require integration of custom data readers into the <b>AccuracyChecker</b> tool. In order to validate results after the Simplified mode the user should rely on own scripts and tools.</p>
<p>In the case of substantial accuracy degradation after applying the <code>DefaultQuantization</code> algorithm there are two alternatives to use:</p><ol type="1">
<li>Hyperparameters tuning</li>
<li>AccuracyAwareQuantization algorithm</li>
<li>Layer-wise hyperparameters tuning</li>
</ol>
<h2>Tuning hyperparameters of the DefaultQuantization</h2>
<p>The default quantization algorithm provides multiple hyperparameters which can be used in order to improve accuracy results for the fully-quantized model. Below is a list of best practices which can be applied to improve accuracy without a substantial performance reduction with respect to default settings:</p><ol type="1">
<li>The first option that we recommend to change is <code>preset</code> that can be varied from <code>performance</code> to <code>mixed</code>. It enables asymmetric quantization of activations and can be helpful for the NNs with non-ReLU activation functions, e.g. YOLO, EfficientNet, etc.</li>
<li>The next option is <code>use_fast_bias</code>. Setting this option for <code>false</code> enables a different bias correction method which is more accurate, in general, and applied after model quantization as a part of <code>DefaultQuantization</code> algorithm. <blockquote class="doxtable">
<p><b>NOTE</b>: Changing this option can substantially increase quantization time in the POT tool. </p>
</blockquote>
</li>
<li>Another important option is a <code>range_estimator</code>. It defines how to calculate the minimum and maximum of quantization range for weights and activations. For example, the following <code>range_estimator</code> for activations can improve the accuracy for Faster R-CNN based networks: <div class="fragment"><div class="line">&quot;compression&quot;: {</div>
<div class="line">    &quot;target_device&quot;: &quot;ANY&quot;,        </div>
<div class="line">    &quot;algorithms&quot;: [</div>
<div class="line">        {</div>
<div class="line">            &quot;name&quot;: &quot;DefaultQuantization&quot;, </div>
<div class="line">            &quot;params&quot;: {</div>
<div class="line">                &quot;preset&quot;: &quot;performance&quot;, </div>
<div class="line">                &quot;stat_subset_size&quot;: 300  </div>
<div class="line">                                         </div>
<div class="line"> </div>
<div class="line">                &quot;activations&quot;: {</div>
<div class="line">                    &quot;range_estimator&quot;: {</div>
<div class="line">                        &quot;max&quot;: {</div>
<div class="line">                            &quot;aggregator&quot;: &quot;max&quot;,</div>
<div class="line">                            &quot;type&quot;: &quot;abs_max&quot;</div>
<div class="line">                        }</div>
<div class="line">                    }</div>
<div class="line">                }</div>
<div class="line">            }</div>
<div class="line">        }</div>
<div class="line">    ]</div>
<div class="line">}</div>
</div><!-- fragment --></li>
</ol>
<p>Please find the possible options and their description in the <code>config/default_quantization_spec.json</code> file in the POT directory.</p>
<ol type="1">
<li>The next option is <code>stat_subset_size</code>. It controls the size of the calibration dataset used by POT to collect statistics for quantization parameters initialization. It is assumed that this dataset should contain a sufficient number of representative samples. Thus, varying this parameter may affect accuracy (the higher is better). However, we empirically found that 300 samples are sufficient to get representative statistics in most cases.</li>
<li>The last option is <code>ignored_scope</code>. It allows excluding some layers from the quantization process, i.e. their inputs will not be quantized. It may be helpful for some patterns for which it is known in advance that they drop accuracy when executing in low-precision. For example, <code>DetectionOutput</code> layer of SSD model expressed as a subgraph should not be quantized to preserve the accuracy of Object Detection models. One of the sources for the ignored scope can be the AccuracyAware algorithm which can revert layers back to the original precision (see details below).</li>
</ol>
<h2>Accuracy-Aware</h2>
<p>In case when the steps above do not lead to the accurate quantized model you may use the so-called <code>AccuracyAwareQuantization</code> algorithm which leads to mixed-precision models. The whole idea behind that is to revert quantized layers back to floating-point precision based on their contribution to the accuracy drop until the desired accuracy degradation with respect to the full-precision model is satisfied.</p>
<p>A fragment of the configuration file with default settings is shown below (<code>configs/accuracy_aware_quantization_template.json</code>): </p><div class="fragment"><div class="line">&quot;compression&quot;: {</div>
<div class="line">        &quot;target_device&quot;: &quot;ANY&quot;, // Target device, the specificity of which will be taken into account during optimization.</div>
<div class="line">                                // The default value &quot;ANY&quot; stands for compatible quantization supported by any HW.</div>
<div class="line">        &quot;algorithms&quot;: [</div>
<div class="line">            {</div>
<div class="line">                &quot;name&quot;: &quot;AccuracyAwareQuantization&quot;, // Optimization algorithm name</div>
<div class="line">                &quot;params&quot;: {</div>
<div class="line">                    &quot;preset&quot;: &quot;performance&quot;, // Preset [performance, mixed, accuracy] which control the quantization</div>
<div class="line">                                             // mode (symmetric, mixed (weights symmetric and activations asymmetric)</div>
<div class="line">                                             // and fully asymmetric respectively)</div>
<div class="line"> </div>
<div class="line">                    &quot;stat_subset_size&quot;: 300, // Size of subset to calculate activations statistics that can be used</div>
<div class="line">                                             // for quantization parameters calculation</div>
<div class="line"> </div>
<div class="line">                    &quot;maximal_drop&quot;: 0.01 // Maximum accuracy drop which has to be achieved after the quantization</div>
<div class="line">                }</div>
<div class="line">            }</div>
<div class="line">        ]</div>
<div class="line">    }</div>
</div><!-- fragment --><p>Since the <code>AccuracyAwareQuantization</code> calls the <code>DefaultQuantization</code> at the first step it means that all the parameters of the latter one are also valid and can be applied to the accuracy-aware scenario.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: In general case, possible speedup after applying the <code>AccuracyAwareQuantization</code> algorithm is less than after the <code>DefaultQuantization</code> when the model gets fully-quantized. </p>
</blockquote>
<p>If you do not achieve the desired accuracy and performance after applying the <code>AccuracyAwareQuantization</code> algorithm or you need an accurate fully-quantized model, we recommend either using layer-wise hyperparameters tuning with TPE or using Quantization-Aware training from <a class="el" href="pot_docs_LowPrecisionOptimizationGuide.html">the supported frameworks</a>.</p>
<h2>Layer-Wise Hyperparameters Tuning Using TPE</h2>
<p>As the last step in post-training optimization, you may try layer-wise hyperparameter tuning using TPE, which stands for Tree of Parzen Estimators hyperparameter optimizer that searches through available configurations trying to find an optimal one. For post-training optimization, TPE assigns multiple available configuration options to choose from for every layer and by evaluating different sets of parameters, it creates a probabilistic model of their impact on accuracy and latency to iteratively find an optimal one.</p>
<p>You can run TPE with any combination of parameters in <code>tuning_scope</code>, but it is recommend using one of two configurations described below. If for some reason, like HW failure or power shutdown, TPE trials stop before completion, you can rerun them starting from the last trial by changing <code>trials_load_method</code> from <code>cold_start</code> to <code>warm_start</code> as long as logs from the previous execution are available.</p>
<blockquote class="doxtable">
<p><b>NOTE</b>: TPE requires many iterations to converge to an optimal solution, and it is recommended to run it for at least 200 iterations. Because every iteration requires evaluation of a generated model , which means accuracy measurements on a dataset and latency measurements using benchmark, this process may take from 24 hours up to few days to complete, depending on a model. To run this configuration on multiple machines and reduce the execution time, see <a class="el" href="pot_compression_optimization_tpe_multinode.html">Multi-node</a>. </p>
</blockquote>
<h3>Range Estimator Configuration</h3>
<p>To run TPE with range estimator tuning, use the following configuration: </p><div class="fragment"><div class="line">&quot;optimizer&quot;: {</div>
<div class="line">    &quot;name&quot;: &quot;Tpe&quot;,</div>
<div class="line">    &quot;params&quot;: {</div>
<div class="line">        &quot;max_trials&quot;: 200,</div>
<div class="line">        &quot;trials_load_method&quot;: &quot;cold_start&quot;,</div>
<div class="line">        &quot;accuracy_loss&quot;: 0.1,</div>
<div class="line">        &quot;latency_reduce&quot;: 1.5,</div>
<div class="line">        &quot;accuracy_weight&quot;: 1.0,</div>
<div class="line">        &quot;latency_weight&quot;: 0.0,</div>
<div class="line">        &quot;benchmark&quot;: {</div>
<div class="line">            &quot;performance_count&quot;: false,</div>
<div class="line">            &quot;batch_size&quot;: 1,</div>
<div class="line">            &quot;nthreads&quot;: 8,</div>
<div class="line">            &quot;nstreams&quot;: 1,</div>
<div class="line">            &quot;nireq&quot;: 1,</div>
<div class="line">            &quot;api_type&quot;: &quot;async&quot;,</div>
<div class="line">            &quot;niter&quot;: 1,</div>
<div class="line">            &quot;duration_seconds&quot;: 30,</div>
<div class="line">            &quot;benchmark_app_dir&quot;: &quot;&lt;path to benchmark_app&gt;&quot; // Path to benchmark_app If not specified, Python base benchmark will be used. Use benchmark_app to reduce jitter in results.</div>
<div class="line">        }</div>
<div class="line">    }</div>
<div class="line">},</div>
<div class="line">&quot;compression&quot;: {</div>
<div class="line">    &quot;target_device&quot;: &quot;ANY&quot;,</div>
<div class="line">    &quot;algorithms&quot;: [</div>
<div class="line">        {</div>
<div class="line">            &quot;name&quot;: &quot;ActivationChannelAlignment&quot;,</div>
<div class="line">            &quot;params&quot;: {</div>
<div class="line">                &quot;stat_subset_size&quot;: 300</div>
<div class="line">            }</div>
<div class="line">        },</div>
<div class="line">        {</div>
<div class="line">            &quot;name&quot;: &quot;TunableQuantization&quot;,</div>
<div class="line">            &quot;params&quot;: {</div>
<div class="line">                &quot;stat_subset_size&quot;: 300,</div>
<div class="line">                &quot;preset&quot;: &quot;performance&quot;,</div>
<div class="line">                &quot;tuning_scope&quot;: [&quot;range_estimator&quot;],</div>
<div class="line">                &quot;estimator_tuning_scope&quot;: [&quot;preset&quot;, &quot;outlier_prob&quot;],</div>
<div class="line">                &quot;outlier_prob_choices&quot;: [1e-3, 1e-4, 1e-5]</div>
<div class="line">            }</div>
<div class="line">        },</div>
<div class="line">        {</div>
<div class="line">            &quot;name&quot;: &quot;FastBiasCorrection&quot;,</div>
<div class="line">            &quot;params&quot;: {</div>
<div class="line">                &quot;stat_subset_size&quot;: 300</div>
<div class="line">            }</div>
<div class="line">        }</div>
<div class="line">    ]</div>
<div class="line">}</div>
</div><!-- fragment --><p>This configuration searches for optimal preset for <code>range_estimator</code> and optimal outlier probability for quantiles for every layer. Because this configuration only changes final values provided to <a href="(https://docs.openvinotoolkit.org/latest/_docs_ops_quantization_FakeQuantize_1.html)">FakeQuantize</a> layers, changes in parameters do not impact inference latency, thus we set <code>latency_weight</code> to 0 to prevent jitter in benchmark results to negatively impact model evaluation. Experiments show that this configuration can give much better accuracy then the approach of just changing <code>range_estimator</code> configuration globally.</p>
<h3>Layer Configuration</h3>
<p>To run TPE with layer tuning, use the following configuration: </p><div class="fragment"><div class="line">&quot;optimizer&quot;: {</div>
<div class="line">    &quot;name&quot;: &quot;Tpe&quot;,</div>
<div class="line">    &quot;params&quot;: {</div>
<div class="line">        &quot;max_trials&quot;: 200,</div>
<div class="line">        &quot;trials_load_method&quot;: &quot;cold_start&quot;,</div>
<div class="line">        &quot;accuracy_loss&quot;: 0.1,</div>
<div class="line">        &quot;latency_reduce&quot;: 1.5,</div>
<div class="line">        &quot;accuracy_weight&quot;: 1.0,</div>
<div class="line">        &quot;latency_weight&quot;: 1.0,</div>
<div class="line">        &quot;benchmark&quot;: {</div>
<div class="line">            &quot;performance_count&quot;: false,</div>
<div class="line">            &quot;batch_size&quot;: 1,</div>
<div class="line">            &quot;nthreads&quot;: 8,</div>
<div class="line">            &quot;nstreams&quot;: 1,</div>
<div class="line">            &quot;nireq&quot;: 1,</div>
<div class="line">            &quot;api_type&quot;: &quot;async&quot;,</div>
<div class="line">            &quot;niter&quot;: 1,</div>
<div class="line">            &quot;duration_seconds&quot;: 30,</div>
<div class="line">            &quot;benchmark_app_dir&quot;: &quot;&lt;path to benchmark_app&gt;&quot; // Path to benchmark_app If not specified, Python base benchmark will be used. Use benchmark_app to reduce jitter in results.</div>
<div class="line">        }</div>
<div class="line">    }</div>
<div class="line">},</div>
<div class="line">&quot;compression&quot;: {</div>
<div class="line">    &quot;target_device&quot;: &quot;ANY&quot;,</div>
<div class="line">    &quot;algorithms&quot;: [</div>
<div class="line">        {</div>
<div class="line">            &quot;name&quot;: &quot;ActivationChannelAlignment&quot;,</div>
<div class="line">            &quot;params&quot;: {</div>
<div class="line">                &quot;stat_subset_size&quot;: 300</div>
<div class="line">            }</div>
<div class="line">        },</div>
<div class="line">        {</div>
<div class="line">            &quot;name&quot;: &quot;TunableQuantization&quot;,</div>
<div class="line">            &quot;params&quot;: {</div>
<div class="line">                &quot;stat_subset_size&quot;: 300,</div>
<div class="line">                &quot;preset&quot;: &quot;performance&quot;,</div>
<div class="line">                &quot;tuning_scope&quot;: [&quot;layer&quot;]</div>
<div class="line">            }</div>
<div class="line">        },</div>
<div class="line">        {</div>
<div class="line">            &quot;name&quot;: &quot;FastBiasCorrection&quot;,</div>
<div class="line">            &quot;params&quot;: {</div>
<div class="line">                &quot;stat_subset_size&quot;: 300</div>
<div class="line">            }</div>
<div class="line">        }</div>
<div class="line">    ]</div>
<div class="line">}</div>
</div><!-- fragment --><p>This configuration is similar to <code>AccuracyAwareQuantization</code>, because it also tries to revert quantized layers back to floating-point precision, but uses a different algorithm to choose layers, which can lead to better results. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
    <div class="footer">
      <div id="nav-path" class="navpath"></div>
      <div class="footer-content">
        <div class="opt-notice">For more complete information about compiler optimizations, see our 
        <a class="el" href="https://software.intel.com/articles/optimization-notice">Optimization Notice</a>
        </div>
        <div class="footer-row">
          <ul>
            <li><a href="https://software.intel.com/en-us/forums/computer-vision">Support</a></li>
            <li><a href="/<version_placeholder>/openvino_docs_Legal_Information.html">Legal Information</a></li>
            <li><a href="https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html">Cookies and Similar Technologies Notice</a></li>
          </ul>
        </div>
      </div>
    </div>
  </body>
</html>